{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YI_ALFRED_MSDS458_FinalAssignment_Part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1LCNAvBrpoU",
        "colab_type": "text"
      },
      "source": [
        "# MSDS 458: Research/Programming Assignment #4 (Final Assignment): Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHHggPPPsizd",
        "colab_type": "text"
      },
      "source": [
        "**Management Problem**\n",
        "\n",
        "For this final research assignment, I apply deep learning methods that we covered in the course (MSDS 458) to conduct sentiment analysis of Twitter data, a challenging yet important field of study for organizations in both public and private sectors. Twitter is a popular platform where entities at all levels—governments, businesses, country leaders, celebrities, and even the average social media user—express their opinions. The content of such tweets could represent a country's official policy or collectively, a country's public sentiment toward a particular issue. Given the massive volume of tweets generated each day—on average, [6,000 tweets are posted on Twitter every second](https://www.internetlivestats.com/twitter-statistics/) —there is immense value in being able quickly and accurately determine such sentiment values (e.g., positive or negative). I combine natural language processing (NLP) and deep learning techniques to build a robust Twitter sentiment classification model. \n",
        "\n",
        "**Corpus Description**\n",
        "\n",
        "The corpus I use is Stanford University's [Sentiment140](http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip). The dataset is a CSV file consisting of 1.6 million English-language tweets. The tweets are annotated using six attributes: 1. polarity of the tweet (0 = negative, 2 = neutral, 4 = positive); 2. ID; 3. date; 4. query; 5. username; 6. text.  For this project, I plan to use the polarity and text content to build a model that could take any given tweet and determine the most probable sentiment value.\n",
        "\n",
        "**Methods**\n",
        "\n",
        "***Text Preprocessing & Data Exploration***\n",
        "<p>Given the massive number of documents in the corpus (one tweet = one document), it is important to ensure the data, particularly each tweet's text, is cleaned and tokenized properly for any follow-on modeling tasks. Regular expressions are useful in handling any emoticons or special characters (e.g., @ symbols and hashtags). As part of the data exploration step, I also generate visualizations using histograms and word clouds representing various aspects of the tweets (e.g., text content, null values, positive/negative/neutral breakdown, etc.) to gain a broad understanding of both qualitative and quantitative aspects of the corpus.</p>\n",
        "\n",
        "***Text Vectorization***\n",
        "<p>While the focus of this project is evaluating optimal deep neural network (DNN) architectures and parameters for Twitter sentiment analysis, such work is dependent on proper experimentation and implementation of various text vectorization methods. I leverage the techniques I learned in MSDS 453 for this task, which include applying word embedding approaches such as TF-IDF and Doc2vec. Since there are 1.6 million documents, it is important for me to limit the vocabulary by tuning the vectorizer hyperparameters (e.g., max_features, max_df) and exploring dimensionality reduction techniques, such as PCA.</p>\n",
        "<p>I evaluate the vectors using various classifiers to ensure I have the optimal number of features in training for the classification models. I experiment with both traditional classifiers (e.g., logistic regression, random forest, and ensemble methods) and neural networks.</p>\n",
        "\n",
        "***Building Neural Network-Based Models***\n",
        "<p>The main part of this research assignment entails careful experimentation of various DNNs we covered in MSDS 458, including fully-connected dense networks, recurrent neural networks (RNN), long short-term memory networks (LSTM), and convolutional neural networks (CNN). The neural networks are built using Keras. The evaluation method to determine the best classification model consists of a strict training-and-test regimen using a crossed experimental design (e.g., maintain consistent hyperparameter settings for consistency). Vocabulary size and word embeddings also remain consistent. I experiment with various network structure designs, hyperparameter settings, and model fit methods by taking the input data (i.e., processed tweet text vectors) through single to multiple layers consisting of varying nodes/units across dense networks, RNNs, LSTM networks, and CNNs.</p>\n",
        "\n",
        "**Evaluation**\n",
        "<p>I track and compare training/testing times as well as accuracy and loss curves for train, validation, and test datasets to evaluate the deep learning models' performances. I also provide charts and plots summarizing these key metrics to visually capture which neural network structures and models yield the strongest performance, in terms of implementation time and accuracy.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozN6nbO9tnWX",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKkcBrRNrbvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import namedtuple\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import utils\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from pprint import pprint\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWi963IVt1ey",
        "colab_type": "code",
        "outputId": "2052ec0b-d555-40a7-d854-6b8ca2cba595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amerue1o5vo1",
        "colab_type": "code",
        "outputId": "49141de9-00fc-4e12-a2bf-04f8bfecd284",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/MSDS Files/MSDS 458/Assignment 4')\n",
        "\n",
        "# Check working directory\n",
        "!pwd\n",
        "\n",
        "# Check files in directory\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MSDS Files/MSDS 458/Assignment 4\n",
            "Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wt4iPv4PP0p",
        "colab_type": "text"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2PuEtQkt3z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['sentiment','id','date','query','user','text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIw56n03t_3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = '/content/drive/My Drive/MSDS Files/MSDS 458/Assignment 4/Data/trainingandtestdata/training.1600000.processed.noemoticon.csv'\n",
        "df_train = pd.read_csv(train_data, encoding = 'ISO-8859-1', header=None, names=cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb9-Lx_RuKS6",
        "colab_type": "code",
        "outputId": "5a3c876d-c1f5-458b-d786-9f4dc98932da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment  ...                                               text\n",
              "0          0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  ...  is upset that he can't update his Facebook by ...\n",
              "2          0  ...  @Kenichan I dived many times for the ball. Man...\n",
              "3          0  ...    my whole body feels itchy and like its on fire \n",
              "4          0  ...  @nationwideclass no, it's not behaving at all....\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceSn78_ouMMk",
        "colab_type": "code",
        "outputId": "a6e7035c-30ce-4d0a-becb-dfcaebfb9af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            " #   Column     Non-Null Count    Dtype \n",
            "---  ------     --------------    ----- \n",
            " 0   sentiment  1600000 non-null  int64 \n",
            " 1   id         1600000 non-null  int64 \n",
            " 2   date       1600000 non-null  object\n",
            " 3   query      1600000 non-null  object\n",
            " 4   user       1600000 non-null  object\n",
            " 5   text       1600000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QriXHNJ_vw4M",
        "colab_type": "code",
        "outputId": "0cf22cd5-09d5-476b-b946-95ae4ddaea72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df_train.loc[1500]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment                               0\n",
              "id                             1468168034\n",
              "date         Tue Apr 07 00:05:12 PDT 2009\n",
              "query                            NO_QUERY\n",
              "user                        RopeMarksMuse\n",
              "text             I am soooo tired @ work \n",
              "Name: 1500, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFlgFHbUQuVs",
        "colab_type": "text"
      },
      "source": [
        "Column Definitions\n",
        "\n",
        "*  0 — the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "*  1 — the id of the tweet (1500)\n",
        "*  2 — the date of the tweet (Tue Apr 07 00:05:12 PDT 2009)\n",
        "*  3 — the query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "*  4 — the user that tweeted (RopeMarksMuse)\n",
        "*  5 — the text of the tweet (I am soooo tired @ work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63HotJo2Qona",
        "colab_type": "code",
        "outputId": "3b098e51-2e4d-46f7-b9aa-20305c8d7b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sentiment_pos_count = df_train.sentiment.value_counts().iloc[0]\n",
        "sentiment_neg_count = df_train.sentiment.value_counts().iloc[1]\n",
        "print(f'Positive Sentiment Count: {sentiment_pos_count}\\nNegative Sentiment Count: {sentiment_neg_count}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Sentiment Count: 800000\n",
            "Negative Sentiment Count: 800000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsXSnnp3V3E_",
        "colab_type": "text"
      },
      "source": [
        "Since there are no \"neutral\" sentiment values, I will change the positive sentiment polarity values from 4 to 1 for simplicity. The negative labels can remain 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkFvbzugWF5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.sentiment = df_train.sentiment.replace(to_replace=4,value=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plH2qfyxXaR_",
        "colab_type": "code",
        "outputId": "c566bb84-e083-464e-c4bf-75607046ae47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df_train.sentiment.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800000\n",
              "0    800000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRKio4oWX7vz",
        "colab_type": "text"
      },
      "source": [
        "Dropping unnecessary columns to simplify dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehINOoJ6XwFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.drop(['id','date','query','user'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBew308KYDCF",
        "colab_type": "code",
        "outputId": "28a8af10-97ae-444c-dd60-da0168aa398f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea6QPDk_avXu",
        "colab_type": "text"
      },
      "source": [
        "Creating new column that indicates character count of each tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rUhv5q4YhoH",
        "colab_type": "code",
        "outputId": "d2afcba0-5bd5-4c6b-8dfc-8eb1dae82d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df_train['text_len'] = [len(t) for t in df_train.text]\n",
        "df_train['text_len']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          115\n",
              "1          111\n",
              "2           89\n",
              "3           47\n",
              "4          111\n",
              "          ... \n",
              "1599995     56\n",
              "1599996     78\n",
              "1599997     57\n",
              "1599998     65\n",
              "1599999     62\n",
              "Name: text_len, Length: 1600000, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HT41-vQY95E",
        "colab_type": "code",
        "outputId": "6ad30b51-2196-4b30-a118-83c52e99a639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_train['text_len'].max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jomEDbpZE53",
        "colab_type": "code",
        "outputId": "bc752201-8bff-4308-ac40-fd30de579504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_train['text_len'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mynBpPJcaEZZ",
        "colab_type": "text"
      },
      "source": [
        "Creating data dictionary to facilitate data exploration and manipulation throughout the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTSWXXM_ZGTI",
        "colab_type": "code",
        "outputId": "f6e861af-8ca8-4d7e-a164-99f51024b132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "data_dict = {\n",
        "    'sentiment':{\n",
        "        'type':df_train.sentiment.dtype,\n",
        "        'description':'sentiment class - 0:negative, 1:positive'\n",
        "    },\n",
        "    'text':{\n",
        "        'type':df_train.text.dtype,\n",
        "        'description':'tweet text'\n",
        "    },\n",
        "    'text_len':{\n",
        "        'type':df_train.text_len.dtype,\n",
        "        'description':'Length of the tweet before processing'\n",
        "    },\n",
        "    'dataset_shape':df_train.shape\n",
        "}\n",
        "pprint(data_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset_shape': (1600000, 3),\n",
            " 'sentiment': {'description': 'sentiment class - 0:negative, 1:positive',\n",
            "               'type': dtype('int64')},\n",
            " 'text': {'description': 'tweet text', 'type': dtype('O')},\n",
            " 'text_len': {'description': 'Length of the tweet before processing',\n",
            "              'type': dtype('int64')}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSvctTQna2EG",
        "colab_type": "text"
      },
      "source": [
        "Checking how many tweets in the dataset contain more than the allowed 140 character count (note: 140 was the maximum character count for the tweets collected in this dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMW2E66FaaAT",
        "colab_type": "code",
        "outputId": "ee06d820-89fe-4a80-f705-e4f5eb0ffa13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "df_train[df_train.text_len > 140].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment    17174\n",
              "text         17174\n",
              "text_len     17174\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33jtlM2Ha9Ht",
        "colab_type": "text"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw-8mXlmhxCa",
        "colab_type": "text"
      },
      "source": [
        "### HTML Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om6m0Jp_-f26",
        "colab_type": "text"
      },
      "source": [
        "Fixing ‘&amp’,’&quot’,etc. encoding issues using BS4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiRDGnFB-Qbs",
        "colab_type": "code",
        "outputId": "d539df4c-af54-4923-86cf-c506143c2997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df_train.text[279]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Whinging. My client&amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;reviewed correctly. \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awKBvyT0-Uyf",
        "colab_type": "code",
        "outputId": "83df6be8-ea48-40a5-a0d5-5804b3e6411c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_01 = BeautifulSoup(df_train.text[279], 'lxml')\n",
        "sample_01.get_text()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Whinging. My client&boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&reviewed correctly. \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm64CDBR-lzu",
        "colab_type": "text"
      },
      "source": [
        "### Removing @ mentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0TKOK2e_a5Z",
        "colab_type": "text"
      },
      "source": [
        "@ mentions indicate a user but for the purposes of this project, this text does not add value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88qipfvQ-Ye0",
        "colab_type": "code",
        "outputId": "ccb64310-f443-49c4-9b57-ebe0b07fdff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df_train.text[3000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@islandiva147 I sent u a tweet yesterday but I don't know why it didn't work  I guess you're sleeping right now I am working soon noon !!!\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjB1B4Ih-4Wk",
        "colab_type": "code",
        "outputId": "db5bb385-d89f-4aaa-8f48-90aacf2b63cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_02 = re.sub(r'@[A-Za-z0-9]+','',df_train.text[3000])\n",
        "sample_02"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I sent u a tweet yesterday but I don't know why it didn't work  I guess you're sleeping right now I am working soon noon !!!\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhXLiGXi_63K",
        "colab_type": "text"
      },
      "source": [
        "### Removing URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwJT5O2sACKb",
        "colab_type": "text"
      },
      "source": [
        "URLs can be useful for other purposes, but similar to the @ mentions, they do not add value in training a model for sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsIbLl6zAP7j",
        "colab_type": "code",
        "outputId": "6a44626a-f28e-410f-c31f-4551d3cbcfcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df_train.text[100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Body Of Missing Northern Calif. Girl Found: Police have found the remains of a missing Northern California girl .. http://tr.im/imji'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REQwrrJW_Ksl",
        "colab_type": "code",
        "outputId": "dd8fd74e-7a48-4977-af8f-123da27cecc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_03 = re.sub('https?://[A-Za-z0-9./]+','',df_train.text[100])\n",
        "sample_03"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Body Of Missing Northern Calif. Girl Found: Police have found the remains of a missing Northern California girl .. '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9x0EZ9r9Ee",
        "colab_type": "text"
      },
      "source": [
        "### Resolving UTF-8 BOM (Byte Order Mark) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pa6e2lqwsaTo",
        "colab_type": "code",
        "outputId": "2cda0618-0524-4a11-eef8-2ffb05e5ddc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df_train.text[226]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tuesdayï¿½ll start with reflection ï¿½n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1QXuPU5jTBL",
        "colab_type": "code",
        "outputId": "3d82e921-b981-4e97-c657-0e58ff088bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_04 = df_train.text[226].encode().decode('utf-8-sig')\n",
        "sample_04"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tuesdayï¿½ll start with reflection ï¿½n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkUfHCu-sjqy",
        "colab_type": "code",
        "outputId": "3a4f9d19-fa34-439b-bd7c-eaf68c35089a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_04.replace(u'ï¿½', '?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tuesday?ll start with reflection ?n then a lecture in Stress reducing techniques. That sure might become very useful for us accompaniers '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsS-QxV8t9Mi",
        "colab_type": "text"
      },
      "source": [
        "### Removing Tweet Hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "084GLGtytvzn",
        "colab_type": "code",
        "outputId": "7d42aeb1-9fd9-408f-d677-fdb4eadf6592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df_train.text[785]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"@hadtobeyou I'm at 900 words, it's all can do  I'll finish tomorrow maybe\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBt8q5kqyjjH",
        "colab_type": "text"
      },
      "source": [
        "Experimenting with various hashtag+symbols removal methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFj96wrpuIx2",
        "colab_type": "code",
        "outputId": "ea86de11-dcb1-495d-dd9d-1d44b28223ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_05 = re.sub('[^a-zA-Z]', ' ', df_train.text[785])\n",
        "sample_05"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hadtobeyou I m at     words  it s all can do  I ll finish tomorrow maybe'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CytDmjW8uSxf",
        "colab_type": "code",
        "outputId": "edf3fb37-31e7-4799-898e-fb000f28e98e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_06 = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", df_train.text[785]).split())\n",
        "sample_06"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I m at 900 words it s all can do I ll finish tomorrow maybe'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpvqMBx_zSid",
        "colab_type": "text"
      },
      "source": [
        "The second method seems to be more effective in cleaning the tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gut5V9lFyvE3",
        "colab_type": "text"
      },
      "source": [
        "### Tweet Cleaner Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nezLr8Lyy7Dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tok = WordPunctTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQcigJPO7ALj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "re1 = r'@[A-Za-z0-9_]+'\n",
        "re2 = r'https?://[^ ]+'\n",
        "combined_re = r'|'.join((re1, re2))\n",
        "www_pat = r'www.[^ ]+'\n",
        "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
        "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
        "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
        "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
        "                \"mustn't\":\"must not\"}\n",
        "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    try:\n",
        "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        bom_removed = souped\n",
        "    stripped = re.sub(combined_re, '', bom_removed)\n",
        "    stripped = re.sub(www_pat, '', stripped)\n",
        "    lower_case = stripped.lower()\n",
        "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
        "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "    # I will tokenize and join together to remove unneccessary white spaces\n",
        "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
        "    return (\" \".join(words)).strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdQqkmnR0KSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaner_test = df_train.text[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rMC8DmL0YbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_result = []\n",
        "for i in cleaner_test:\n",
        "  test_result.append(tweet_cleaner(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUhrhqsV0gmD",
        "colab_type": "code",
        "outputId": "247c46ed-b4c2-4821-a457-03dc6e0863e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['awww that bummer you shoulda got david carr of third day to do it',\n",
              " 'is upset that he can not update his facebook by texting it and might cry as result school today also blah',\n",
              " 'dived many times for the ball managed to save the rest go out of bounds',\n",
              " 'my whole body feels itchy and like its on fire',\n",
              " 'no it not behaving at all mad why am here because can not see you all over there',\n",
              " 'not the whole crew',\n",
              " 'need hug',\n",
              " 'hey long time no see yes rains bit only bit lol fine thanks how you',\n",
              " 'nope they did not have it',\n",
              " 'que me muera',\n",
              " 'spring break in plain city it snowing',\n",
              " 'just re pierced my ears',\n",
              " 'could not bear to watch it and thought the ua loss was embarrassing',\n",
              " 'it it counts idk why did either you never talk to me anymore',\n",
              " 'would ve been the first but did not have gun not really though zac snyder just doucheclown',\n",
              " 'wish got to watch it with you miss you and how was the premiere',\n",
              " 'hollis death scene will hurt me severely to watch on film wry is directors cut not out now',\n",
              " 'about to file taxes',\n",
              " 'ahh ive always wanted to see rent love the soundtrack',\n",
              " 'oh dear were you drinking out of the forgotten table drinks',\n",
              " 'was out most of the day so did not get much done',\n",
              " 'one of my friend called me and asked to meet with her at mid valley today but ve no time sigh',\n",
              " 'baked you cake but ated it',\n",
              " 'this week is not going as had hoped',\n",
              " 'blagh class at tomorrow',\n",
              " 'hate when have to call and wake people up',\n",
              " 'just going to cry myself to sleep after watching marley and me',\n",
              " 'im sad now miss lilly',\n",
              " 'ooooh lol that leslie and ok will not do it again so leslie will not get mad again',\n",
              " 'meh almost lover is the exception this track gets me depressed every time',\n",
              " 'some hacked my account on aim now have to make new one',\n",
              " 'want to go to promote gear and groove but unfornately no ride there may going to the one in anaheim in may though',\n",
              " 'thought sleeping in was an option tomorrow but realizing that it now is not evaluations in the morning and work in the afternoon',\n",
              " 'awe love you too am here miss you',\n",
              " 'cry my asian eyes to sleep at night',\n",
              " 'ok sick and spent an hour sitting in the shower cause was too sick to stand and held back the puke like champ bed now',\n",
              " 'ill tell ya the story later not good day and ill be workin for like three more hours',\n",
              " 'sorry bed time came here gmt',\n",
              " 'do not either its depressing do not think even want to know about the kids in suitcases',\n",
              " 'bed class work gym or then class another day that gonna fly by miss my girlfriend',\n",
              " 'really do not feel like getting up today but got to study to for tomorrows practical exam',\n",
              " 'he the reason for the teardrops on my guitar the only one who has enough of me to break my heart',\n",
              " 'sad sad sad do not know why but hate this feeling wanna sleep and still can not',\n",
              " 'soo wish was there to see you finally comfortable im sad that missed it',\n",
              " 'falling asleep just heard about that tracy girl body being found how sad my heart breaks for that family',\n",
              " 'yay happy for you with your job but that also means less time for me and you',\n",
              " 'just checked my user timeline on my blackberry it looks like the twanking is still happening are ppl still having probs bgs and uids',\n",
              " 'oh man was ironing fave top to wear to meeting burnt it',\n",
              " 'is strangely sad about lilo and samro breaking up',\n",
              " 'oh so sorry did not think about that before retweeting',\n",
              " 'broadband plan massive broken promise via still waiting for broadband we are',\n",
              " 'wow tons of replies from you may have to unfollow so can see my friends tweets you re scrolling the feed lot',\n",
              " 'our duck and chicken are taking wayyy too long to hatch',\n",
              " 'put vacation photos online few yrs ago pc crashed and now forget the name of the site',\n",
              " 'need hug',\n",
              " 'not sure what they are only that they are pos as much as want to dont think can trade away company assets sorry andy',\n",
              " 'hate when that happens',\n",
              " 'have sad feeling that dallas is not going to show up gotta say though you think more shows would use music from the game mmm',\n",
              " 'ugh degrees tomorrow',\n",
              " 'where did move to thought were already in sd hmmm random found me glad to hear yer doing well',\n",
              " 'miss my ps it out of commission wutcha playing have you copped blood on the sand',\n",
              " 'just leaving the parking lot of work',\n",
              " 'the life is cool but not for me',\n",
              " 'sadly though ve never gotten to experience the post coitus cigarette before and now never will',\n",
              " 'had such nice day too bad the rain comes in tomorrow at am',\n",
              " 'too bad will not be around lost my job and can not even pay my phone bill lmao aw shucks',\n",
              " 'damm back to school tomorrow',\n",
              " 'mo jobs no money how in the hell is min wage here clams an hour',\n",
              " 'not forever see you soon',\n",
              " 'agreed saw the failwhale allllll day today',\n",
              " 'oh haha dude dont really look at em unless someone says hey added you sorry so terrible at that need pop up',\n",
              " 'sure you re right need to start working out with you and the nikster or jared at least',\n",
              " 'really hate how people diss my bands trace is clearly not ugly',\n",
              " 'gym attire today was puma singlet adidas shorts and black business socks and leather shoes lucky did not run into any cute girls',\n",
              " 'why will not you show my location',\n",
              " 'no picnic my phone smells like citrus',\n",
              " 'my donkey is sensitive about such comments nevertheless he and me be glad to see your mug asap charger is still awol',\n",
              " 'no new csi tonight fml',\n",
              " 'think my arms are sore from tennis',\n",
              " 'wonders why someone that like so much can make you so unhappy in split seccond depressed',\n",
              " 'sleep soon just hate saying bye and see you tomorrow for the night',\n",
              " 'just got ur newsletter those fares really are unbelievable shame already booked and paid for mine',\n",
              " 'missin the boo',\n",
              " 'me too itm',\n",
              " 'damn do not have any chalk my chalkboard is useless',\n",
              " 'had blast at the getty villa but hates that she had sore throat all day it just getting worse too',\n",
              " 'hey missed ya at the meeting sup mama',\n",
              " 'my tummy hurts wonder if the hypnosis has anything to do with it if so it working get it stop smoking',\n",
              " 'why is it always the fat ones',\n",
              " 'sorry babe my fam annoys me too thankfully they re asleep right now muahaha evil laugh',\n",
              " 'should have paid more attention when we covered photoshop in my webpage design class in undergrad',\n",
              " 'wednesday my day do not know what do',\n",
              " 'poor cameron the hills',\n",
              " 'pray for me please the ex is threatening to start sh at my our babies st birthday party what jerk and still have headache',\n",
              " 'hmm do really enjoy being with him if the problems are too constants should think things more find someone ulike',\n",
              " 'strider is sick little puppy',\n",
              " 'so rylee grace wana go steve party or not sadly since its easter wnt able do much but ohh well',\n",
              " 'hey actually won one of my bracket pools too bad it was not the one for money',\n",
              " 'you do not follow me either and work for you',\n",
              " 'bad nite for the favorite teams astros and spartans lose the nite out with was good']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uscOCz60uaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_split = [0, 400000, 800000, 1200000, 1600000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3WbBsBo1CpA",
        "colab_type": "code",
        "outputId": "be27b472-4dd1-44bc-a346-4a00a197ffc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "%%time\n",
        "print(\"Cleaning tweets...\\n\")\n",
        "clean_tweet_text = []\n",
        "for i in range(df_train_split[0],df_train_split[1]):\n",
        "    if( (i+1)%10000 == 0 ):                \n",
        "        print(\"Tweets %d of %d has been processed\" % ( i+1, df_train_split[1] ))                                                  \n",
        "    clean_tweet_text.append(tweet_cleaner(df_train.text[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning tweets...\n",
            "\n",
            "Tweets 10000 of 400000 has been processed\n",
            "Tweets 20000 of 400000 has been processed\n",
            "Tweets 30000 of 400000 has been processed\n",
            "Tweets 40000 of 400000 has been processed\n",
            "Tweets 50000 of 400000 has been processed\n",
            "Tweets 60000 of 400000 has been processed\n",
            "Tweets 70000 of 400000 has been processed\n",
            "Tweets 80000 of 400000 has been processed\n",
            "Tweets 90000 of 400000 has been processed\n",
            "Tweets 100000 of 400000 has been processed\n",
            "Tweets 110000 of 400000 has been processed\n",
            "Tweets 120000 of 400000 has been processed\n",
            "Tweets 130000 of 400000 has been processed\n",
            "Tweets 140000 of 400000 has been processed\n",
            "Tweets 150000 of 400000 has been processed\n",
            "Tweets 160000 of 400000 has been processed\n",
            "Tweets 170000 of 400000 has been processed\n",
            "Tweets 180000 of 400000 has been processed\n",
            "Tweets 190000 of 400000 has been processed\n",
            "Tweets 200000 of 400000 has been processed\n",
            "Tweets 210000 of 400000 has been processed\n",
            "Tweets 220000 of 400000 has been processed\n",
            "Tweets 230000 of 400000 has been processed\n",
            "Tweets 240000 of 400000 has been processed\n",
            "Tweets 250000 of 400000 has been processed\n",
            "Tweets 260000 of 400000 has been processed\n",
            "Tweets 270000 of 400000 has been processed\n",
            "Tweets 280000 of 400000 has been processed\n",
            "Tweets 290000 of 400000 has been processed\n",
            "Tweets 300000 of 400000 has been processed\n",
            "Tweets 310000 of 400000 has been processed\n",
            "Tweets 320000 of 400000 has been processed\n",
            "Tweets 330000 of 400000 has been processed\n",
            "Tweets 340000 of 400000 has been processed\n",
            "Tweets 350000 of 400000 has been processed\n",
            "Tweets 360000 of 400000 has been processed\n",
            "Tweets 370000 of 400000 has been processed\n",
            "Tweets 380000 of 400000 has been processed\n",
            "Tweets 390000 of 400000 has been processed\n",
            "Tweets 400000 of 400000 has been processed\n",
            "CPU times: user 2min 49s, sys: 10.8 s, total: 3min\n",
            "Wall time: 3min 37s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK4F2C1L1POt",
        "colab_type": "code",
        "outputId": "83d3ff2f-2c90-4786-8ec2-d784485a1238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(clean_tweet_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LLFd5MV2XEI",
        "colab_type": "code",
        "outputId": "ad8f6b71-fcf9-4caa-8230-b0b940d196fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "%%time\n",
        "print(\"Cleaning tweets...\\n\")\n",
        "for i in range(df_train_split[1],df_train_split[2]):\n",
        "    if( (i+1)%10000 == 0 ):                \n",
        "        print(\"Tweets %d of %d has been processed\" % ( i+1, df_train_split[2] ))                                                  \n",
        "    clean_tweet_text.append(tweet_cleaner(df_train.text[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning tweets...\n",
            "\n",
            "Tweets 410000 of 800000 has been processed\n",
            "Tweets 420000 of 800000 has been processed\n",
            "Tweets 430000 of 800000 has been processed\n",
            "Tweets 440000 of 800000 has been processed\n",
            "Tweets 450000 of 800000 has been processed\n",
            "Tweets 460000 of 800000 has been processed\n",
            "Tweets 470000 of 800000 has been processed\n",
            "Tweets 480000 of 800000 has been processed\n",
            "Tweets 490000 of 800000 has been processed\n",
            "Tweets 500000 of 800000 has been processed\n",
            "Tweets 510000 of 800000 has been processed\n",
            "Tweets 520000 of 800000 has been processed\n",
            "Tweets 530000 of 800000 has been processed\n",
            "Tweets 540000 of 800000 has been processed\n",
            "Tweets 550000 of 800000 has been processed\n",
            "Tweets 560000 of 800000 has been processed\n",
            "Tweets 570000 of 800000 has been processed\n",
            "Tweets 580000 of 800000 has been processed\n",
            "Tweets 590000 of 800000 has been processed\n",
            "Tweets 600000 of 800000 has been processed\n",
            "Tweets 610000 of 800000 has been processed\n",
            "Tweets 620000 of 800000 has been processed\n",
            "Tweets 630000 of 800000 has been processed\n",
            "Tweets 640000 of 800000 has been processed\n",
            "Tweets 650000 of 800000 has been processed\n",
            "Tweets 660000 of 800000 has been processed\n",
            "Tweets 670000 of 800000 has been processed\n",
            "Tweets 680000 of 800000 has been processed\n",
            "Tweets 690000 of 800000 has been processed\n",
            "Tweets 700000 of 800000 has been processed\n",
            "Tweets 710000 of 800000 has been processed\n",
            "Tweets 720000 of 800000 has been processed\n",
            "Tweets 730000 of 800000 has been processed\n",
            "Tweets 740000 of 800000 has been processed\n",
            "Tweets 750000 of 800000 has been processed\n",
            "Tweets 760000 of 800000 has been processed\n",
            "Tweets 770000 of 800000 has been processed\n",
            "Tweets 780000 of 800000 has been processed\n",
            "Tweets 790000 of 800000 has been processed\n",
            "Tweets 800000 of 800000 has been processed\n",
            "CPU times: user 2min 49s, sys: 11.9 s, total: 3min 1s\n",
            "Wall time: 3min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9RZIb4o2yVt",
        "colab_type": "code",
        "outputId": "326a4f63-f619-4312-e8b4-e91ada0976b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(clean_tweet_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "800000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAStAiMX3uhD",
        "colab_type": "code",
        "outputId": "3802c4a8-505c-4f80-8225-d077e531956e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "%%time\n",
        "print(\"Cleaning tweets...\\n\")\n",
        "for i in range(df_train_split[2],df_train_split[3]):\n",
        "    if( (i+1)%10000 == 0 ):                \n",
        "        print(\"Tweets %d of %d has been processed\" % ( i+1, df_train_split[3] ))                                                  \n",
        "    clean_tweet_text.append(tweet_cleaner(df_train.text[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning tweets...\n",
            "\n",
            "Tweets 810000 of 1200000 has been processed\n",
            "Tweets 820000 of 1200000 has been processed\n",
            "Tweets 830000 of 1200000 has been processed\n",
            "Tweets 840000 of 1200000 has been processed\n",
            "Tweets 850000 of 1200000 has been processed\n",
            "Tweets 860000 of 1200000 has been processed\n",
            "Tweets 870000 of 1200000 has been processed\n",
            "Tweets 880000 of 1200000 has been processed\n",
            "Tweets 890000 of 1200000 has been processed\n",
            "Tweets 900000 of 1200000 has been processed\n",
            "Tweets 910000 of 1200000 has been processed\n",
            "Tweets 920000 of 1200000 has been processed\n",
            "Tweets 930000 of 1200000 has been processed\n",
            "Tweets 940000 of 1200000 has been processed\n",
            "Tweets 950000 of 1200000 has been processed\n",
            "Tweets 960000 of 1200000 has been processed\n",
            "Tweets 970000 of 1200000 has been processed\n",
            "Tweets 980000 of 1200000 has been processed\n",
            "Tweets 990000 of 1200000 has been processed\n",
            "Tweets 1000000 of 1200000 has been processed\n",
            "Tweets 1010000 of 1200000 has been processed\n",
            "Tweets 1020000 of 1200000 has been processed\n",
            "Tweets 1030000 of 1200000 has been processed\n",
            "Tweets 1040000 of 1200000 has been processed\n",
            "Tweets 1050000 of 1200000 has been processed\n",
            "Tweets 1060000 of 1200000 has been processed\n",
            "Tweets 1070000 of 1200000 has been processed\n",
            "Tweets 1080000 of 1200000 has been processed\n",
            "Tweets 1090000 of 1200000 has been processed\n",
            "Tweets 1100000 of 1200000 has been processed\n",
            "Tweets 1110000 of 1200000 has been processed\n",
            "Tweets 1120000 of 1200000 has been processed\n",
            "Tweets 1130000 of 1200000 has been processed\n",
            "Tweets 1140000 of 1200000 has been processed\n",
            "Tweets 1150000 of 1200000 has been processed\n",
            "Tweets 1160000 of 1200000 has been processed\n",
            "Tweets 1170000 of 1200000 has been processed\n",
            "Tweets 1180000 of 1200000 has been processed\n",
            "Tweets 1190000 of 1200000 has been processed\n",
            "Tweets 1200000 of 1200000 has been processed\n",
            "CPU times: user 2min 48s, sys: 10.8 s, total: 2min 59s\n",
            "Wall time: 3min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVBK1uHB3zGp",
        "colab_type": "code",
        "outputId": "a278a3c4-8ac7-424a-f3af-e96682039cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(clean_tweet_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1200000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAOW9Mi930OZ",
        "colab_type": "code",
        "outputId": "d5372462-6693-49b9-af36-42bcd246b8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "%%time\n",
        "print(\"Cleaning tweets...\\n\")\n",
        "for i in range(df_train_split[3],df_train_split[4]):\n",
        "    if( (i+1)%10000 == 0 ):                \n",
        "        print(\"Tweets %d of %d has been processed\" % ( i+1, df_train_split[4] ))                                                  \n",
        "    clean_tweet_text.append(tweet_cleaner(df_train.text[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaning tweets...\n",
            "\n",
            "Tweets 1210000 of 1600000 has been processed\n",
            "Tweets 1220000 of 1600000 has been processed\n",
            "Tweets 1230000 of 1600000 has been processed\n",
            "Tweets 1240000 of 1600000 has been processed\n",
            "Tweets 1250000 of 1600000 has been processed\n",
            "Tweets 1260000 of 1600000 has been processed\n",
            "Tweets 1270000 of 1600000 has been processed\n",
            "Tweets 1280000 of 1600000 has been processed\n",
            "Tweets 1290000 of 1600000 has been processed\n",
            "Tweets 1300000 of 1600000 has been processed\n",
            "Tweets 1310000 of 1600000 has been processed\n",
            "Tweets 1320000 of 1600000 has been processed\n",
            "Tweets 1330000 of 1600000 has been processed\n",
            "Tweets 1340000 of 1600000 has been processed\n",
            "Tweets 1350000 of 1600000 has been processed\n",
            "Tweets 1360000 of 1600000 has been processed\n",
            "Tweets 1370000 of 1600000 has been processed\n",
            "Tweets 1380000 of 1600000 has been processed\n",
            "Tweets 1390000 of 1600000 has been processed\n",
            "Tweets 1400000 of 1600000 has been processed\n",
            "Tweets 1410000 of 1600000 has been processed\n",
            "Tweets 1420000 of 1600000 has been processed\n",
            "Tweets 1430000 of 1600000 has been processed\n",
            "Tweets 1440000 of 1600000 has been processed\n",
            "Tweets 1450000 of 1600000 has been processed\n",
            "Tweets 1460000 of 1600000 has been processed\n",
            "Tweets 1470000 of 1600000 has been processed\n",
            "Tweets 1480000 of 1600000 has been processed\n",
            "Tweets 1490000 of 1600000 has been processed\n",
            "Tweets 1500000 of 1600000 has been processed\n",
            "Tweets 1510000 of 1600000 has been processed\n",
            "Tweets 1520000 of 1600000 has been processed\n",
            "Tweets 1530000 of 1600000 has been processed\n",
            "Tweets 1540000 of 1600000 has been processed\n",
            "Tweets 1550000 of 1600000 has been processed\n",
            "Tweets 1560000 of 1600000 has been processed\n",
            "Tweets 1570000 of 1600000 has been processed\n",
            "Tweets 1580000 of 1600000 has been processed\n",
            "Tweets 1590000 of 1600000 has been processed\n",
            "Tweets 1600000 of 1600000 has been processed\n",
            "CPU times: user 2min 47s, sys: 11.6 s, total: 2min 59s\n",
            "Wall time: 3min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJspk0aL33zR",
        "colab_type": "code",
        "outputId": "e2451a6c-fac7-467e-c77e-3ed5fc90dd28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(clean_tweet_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAp0UW5-6CL2",
        "colab_type": "text"
      },
      "source": [
        "## Saving Processed Tweets as New CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah_ck3sxBCaC",
        "colab_type": "code",
        "outputId": "f5ccd418-8ec3-472e-9e73-232c90903d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train_clean = pd.DataFrame(clean_tweet_text,columns=['text'])\n",
        "df_train_clean['target'] = df_train.sentiment\n",
        "df_train_clean.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>awww that bummer you shoulda got david carr of...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can not update his facebook b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dived many times for the ball managed to save ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no it not behaving at all mad why am here beca...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  target\n",
              "0  awww that bummer you shoulda got david carr of...       0\n",
              "1  is upset that he can not update his facebook b...       0\n",
              "2  dived many times for the ball managed to save ...       0\n",
              "3     my whole body feels itchy and like its on fire       0\n",
              "4  no it not behaving at all mad why am here beca...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgZxjxOeBLTU",
        "colab_type": "code",
        "outputId": "246609a2-20cd-479d-ec2d-212ba9b65494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "df_train_clean.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   text    1600000 non-null  object\n",
            " 1   target  1600000 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 24.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17XhG9_GC60x",
        "colab_type": "text"
      },
      "source": [
        "Checking for null values. No null values, but empty strings (' ')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GY4e_tXBPh4",
        "colab_type": "code",
        "outputId": "d716eea3-80a1-48d2-dc0a-49eff33fe5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "df_train_clean[df_train_clean.isnull().any(axis=1)].head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, target]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N63jdCKVBfBV",
        "colab_type": "code",
        "outputId": "02a96d7c-eef1-4d82-f7aa-b30ad772a1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(df_train_clean.isnull().any(axis=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3_mSAAjBoP5",
        "colab_type": "code",
        "outputId": "9ff44289-c593-48ab-ee57-5b0e948b79bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df_train_clean[df_train_clean['text'] == ''].index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Int64Index([    208,     249,     282,     398,     430,    1011,    1014,\n",
              "               1231,    1421,    1486,\n",
              "            ...\n",
              "            1596542, 1596670, 1597191, 1597326, 1597684, 1598192, 1598272,\n",
              "            1599247, 1599494, 1599993],\n",
              "           dtype='int64', length=3959)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfi0cd4_CgXv",
        "colab_type": "code",
        "outputId": "ee7e2115-9be8-436e-d04f-8ee6197e8db9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(df_train.text[1421])\n",
        "print(df_train.text[1486])\n",
        "print(df_train.text[1597684])\n",
        "print(df_train.text[1599494])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@marlonjenglish \n",
            "@oishiieats \n",
            "@patty4sound http://twitpic.com/7iuns - \n",
            "@Sworn4DaBosses \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48WV7TybDS_s",
        "colab_type": "text"
      },
      "source": [
        "The empty strings represent tweets text that only contain @mentions and/or URLs. As previously stated, this data does not contribute to sentiment analysis training models so I will drop the records from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAXAoq5Dk-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_clean['text'].replace('', np.nan, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybiW668PFTeh",
        "colab_type": "code",
        "outputId": "6886a0c9-ad54-4c75-d2a1-87bff9cbcbd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(df_train_clean.isnull().any(axis=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4i6354TFYZx",
        "colab_type": "code",
        "outputId": "d3e14c77-de04-4955-b933-fc9761c903d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(df_train_clean.text[1421])\n",
        "print(df_train_clean.text[1486])\n",
        "print(df_train_clean.text[1597684])\n",
        "print(df_train_clean.text[1599494])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJgr4OqOFj8K",
        "colab_type": "code",
        "outputId": "2bd844fa-a9ac-45fd-bb31-4dcdf765ecc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "df_train_clean.dropna(inplace=True)\n",
        "df_train_clean.reset_index(drop=True,inplace=True)\n",
        "df_train_clean.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1596041 entries, 0 to 1596040\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   text    1596041 non-null  object\n",
            " 1   target  1596041 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 24.4+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQX5f00e4idX",
        "colab_type": "code",
        "outputId": "20ce89ca-493e-497c-a12a-16e6834e73cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "df_train_clean.to_csv('clean_tweets.csv',encoding='utf-8')\n",
        "csv = 'clean_tweets.csv'\n",
        "my_df = pd.read_csv(csv,index_col=0)\n",
        "my_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  mask |= (ar1 == a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>awww that bummer you shoulda got david carr of...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is upset that he can not update his facebook b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dived many times for the ball managed to save ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>no it not behaving at all mad why am here beca...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  target\n",
              "0  awww that bummer you shoulda got david carr of...       0\n",
              "1  is upset that he can not update his facebook b...       0\n",
              "2  dived many times for the ball managed to save ...       0\n",
              "3     my whole body feels itchy and like its on fire       0\n",
              "4  no it not behaving at all mad why am here beca...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfJm4eh0FvTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}